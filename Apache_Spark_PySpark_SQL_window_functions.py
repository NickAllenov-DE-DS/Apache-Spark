# -*- coding: utf-8 -*-
"""Spark_hometask_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/127pwJx1qDR8Q5vIBVcNbFzzxA_NH8ePo

# Урок 3. Инструменты работы и визуализации ч.2
Условие: есть набор данных о продажах продуктов с информацией о дате продаж, категории продукта, количестве и выручке от продаж.

Используя Apache Spark, загрузите предоставленный набор данных в DataFrame.

("2023-11-20", "Electronics", 100, 12000),

("2023-11-21", "Electronics", 110, 13000),

("2023-11-22", "Electronics", 105, 12500),

("2023-11-20", "Clothing", 300, 15000),

("2023-11-21", "Clothing", 280, 14000),

("2023-11-22", "Clothing", 320, 16000),

("2023-11-20", "Books", 150, 9000),

("2023-11-21", "Books", 200, 12000),

("2023-11-22", "Books", 180, 10000)

Столбцы: "date", "category", "quantity", "revenue".

С использованием оконных функций, рассчитайте среднее выручки от продаж для каждой категории продукта.
Примените операцию pivot для того, чтобы преобразовать полученные данные таким образом, чтобы в качестве строк были категории продуктов, в качестве столбцов были дни, а значениями были средние значения выручки от продаж за соответствующий день.
"""

# Импорт библиотек и модулей

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, avg, mean, round
from pyspark.sql.window import Window

# Проверка на существование запущенной spark-сессии

if 'spark' in locals():
  spark.stop()

# Создание сессии

spark = SparkSession.builder\
  .appName("SalesAnalysis")\
  .getOrCreate()

# Данные

data = [
    ("2023-11-20", "Electronics", 100, 12000),
    ("2023-11-21", "Electronics", 110, 13000),
    ("2023-11-22", "Electronics", 105, 12500),
    ("2023-11-20", "Clothing", 300, 15000),
    ("2023-11-21", "Clothing", 280, 14000),
    ("2023-11-22", "Clothing", 320, 16000),
    ("2023-11-20", "Books", 150, 9000),
    ("2023-11-21", "Books", 200, 12000),
    ("2023-11-22", "Books", 180, 10000)
]

# Определение схемы

columns = ["date", "category", "quantity", "revenue"]

# # Или явное ручное задание схемы (с допущением нулевых значений)

# from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType

# columns = StructType([
#     StructField("date", StringType, True),
#     StructField("category", StringType, True),
#     StructField("quantity", IntegerType, True),
#     StructField("revenue", IntegerType, True)
# ])

df = spark.createDataFrame(data, schema=columns)

# # В задании столбец "дата" имеет строковый формат, но можно трансформировать её в формат даты:

# df_temp = spark.createDataFrame(data, schema=columns)

# df = df_temp.withColumn("date", to_date(col("date"), yyyy-MM-dd))

df.show()

# Определение оконной спецификации

window_spec = Window.partitionBy("category")

# Использование оконной функции для расчета среднего значения выручки по категориям
# с округлением до 4х знаков после запятой

df_avg_rev = df.withColumn("avg_revenue", round(mean("revenue").over(window_spec), 4))

# Вывод окна рассчетов

df_avg_rev.show()

# Удаление дублирующих строк, оставляя уникальные комбинации category, avg_revenue
df_cat_rev = df_avg_rev.select("category", "avg_revenue").distinct()

df_cat_rev.show()

# Или можно объединить эти 2 строки с округлением до 2х знаков после запятой

df_cat_rev_2 = df.withColumn("avg_revenue", round(mean("revenue")\
  .over(window_spec), 2))\
  .select("category", "avg_revenue").distinct()

df_cat_rev_2.show()

# Применение pivot для преобразования данных

pivot_df = df_avg_rev.groupBy("category").pivot("date").avg("revenue")

# Показать результат

pivot_df.show()

# Останавливаем Spark-сессию

spark.stop()